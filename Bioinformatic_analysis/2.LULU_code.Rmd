---
title: "LULU pipeline of Navalacruz 2021 dataset sampling"
author: "Amara Santiesteban Serrano"
date: " r format(Sys.time(), '%d %B, %Y') "
output: 
  html_document:
    df_print: paged
    pdf_document: default
    word_document: default
  editor_options:
    chunk_output_type: inline
---

*Intermediate step between DADA2 and the data analysis in phyloseq*

```{r Packages}
#install.packages("pacman")
pacman::p_load("ggplot2","tidyr","dplyr","tidyverse","readxl","pastecs","lattice","forcats","corrgram","corrplot","HH","effects","car","multcompView","lme4", "devtools","emmeans","factoextra","ggfortify","RColorBrewer","MuMIn","scales","multifunc", "ggthemes", "knitr", "BiocManager", "phyloseq", "phangorn", "DECIPHER", "biomformat", "rbiom")
```

The *LULU algorithm* is a post-clustering curation method aiming at removing erroneous OTUs to achieve meaningful diversity metrics (e.g.: reduce overestimated species richness).
## Here we will work with ASV as OTUs.

El fundamento, lo que va a "buscar", son los duplicados de PCR (mutación durante el proceso de amplificación)--> no tanto para las variaciones reales que tengan las secuencias en la realidad sino para la variación generada por el proceso de amplificar. Los tres pilares de LULU son: 

*Similitud*: Va a buscar secuencias similares en un 97%
*Co-ocurrencia*: para que se le asignen las secuencias hijas a una secuencia madre, la secuencia madre tiene que estar presente
*Abundancia*: la secuencia madre siempre tiene que ser más abundante que las secuencias hijas

```{r LULU install}
library(devtools)
install_github("tobiasgf/lulu")  
```

```{r load OTU table}
#The requirements of LULU are that the OTU table has samples as columns and OTUs as rows, and that it has unique OTU id's as row names.

OTU_table <- read.table("Raw_data_firefungi_soil/dada2_output/asv_table.csv",sep=',', row.names = 1)
colnames(OTU_table) <- OTU_table[1, ]
OTU_table <- OTU_table[-1, ]
OTU_table[] <- lapply(OTU_table, as.numeric) #We need all the values as numbers, no <chr<
head(OTU_table)
class(OTU_table)
```

```{r produce a match list}
#This step requires a Linux processing. 
#There are different tools such as BLASTn or VSEARCH (see the LULU github for more). The only requirements is that the match list has three columns with pair wise similarity scores for the OTUs. The first column contains the id of the query OTU, the second column contains the id of the matching OTU, and the third column contains the similarity score (%) of the two OTUs.

## Download **Anaconda**
## Open the Anaconda Navigator --> Open the **Powershell Prompt**
## Type: ls --> This will identify the directory 

# VSEARCH: 
# 1) Download the zip file for Windows: https://github.com/torognes/vsearch/releases/download/v2.28.1/vsearch-2.28.1-win-x86_64.zip
# 2) Unzip the file --> find the .exe in the bin folder --> move the .exe to the identified directory
# 3) Type in the Powershell Prompt (replacing "asv_OTU.fasta" for your file name, which should be placed in the SAME DIRECTORY): ./vsearch --usearch_global asv_OTU.fasta --db asv_OTU.fasta --self --id .84 --iddef 1 --userout match_list.txt -userfields query+target+id --maxaccepts 0 --query_cov .9 --maxhits 10 
# 4) The match list should be in the identified directory (.txt file)

matchlist <- read.table("match_list.txt")
```

```{r LULU curation}
curated_result <- lulu::lulu(OTU_table, matchlist)

output_folder <- "Raw_data_firefungi_soil/dada2_output"

#The curated OTU table
curated_asv_table <- curated_result$curated_table
names(curated_asv_table)
curated_result_file <- file.path(output_folder, "curated_result.csv")
write.table(curated_asv_table, curated_result_file, sep=",", quote=F, col.names=NA)

#The original table
curated_result$original_table
#Number of OTUs retained
curated_result$curated_count
#IDs of OTUs retained (list only first)
length(curated_result$curated_otus)
#number of OTUs discarded
curated_result$discarded_count
#IDs of OTUs discarded (list only first)
head(curated_result$discarded_otus)

#Check the computational time
curated_result$runtime
#Check which setting was used for minimum_match
curated_result$minimum_match
#Check which setting was used for minimum_relative_cooccurence
curated_result$minimum_relative_cooccurence

### Check how the OTUs were mapped
#total - total read count
#spread - the number of samples the OTU is present in
#parent_id - ID of OTU with which this OTU was merged (or self)
#curated - ("parent" or "merged"), was this OTU kept as a valid OTU (parent) or merged with another
#rank - The rank of the OTU in terms of decreasing spread and read count
otu_map <- (curated_result$otu_map)
```

```{r Assing taxonomy}
#For the next step, we need that our taxonomy table matches the curated asv table. To that end, we need to cut out the taxonomy table to only the asv present in the curated table: 

taxa2 <- read.table("F:/asv_tax.csv",sep=',', row.names = 1)
colnames(taxa2) <- taxa2[1, ]
taxa2 <- taxa2[-1, ]

common_asv <- intersect(row.names(curated_asv_table), row.names(taxa2))
common_asv
curated_asv_table <- as.matrix(curated_asv_table[common_asv, ])
taxa <- as.matrix(taxa2[common_asv, ])
```

The *LULU function also produces a log file* (named something like lulu.log_20171113_162157) which will be placed in the working directory.
For each OTU processed, the log file contains:

1) A list of all hits, i.e. other OTUs with a sequence similarity above the selected threshold) in the dataset is listed, and
2) all potential parents, i.e. hits with a lower rank number, i.e. higher spread and total read count, and satisfying the selected ratio of read counts, and
3) relative co-occurence of all parents (until a parent satisfying the minimum relative cooccurence (and min avg abundance) thresholds is met, if one such is present), and
4) min avg abundance of parents satisfying minimum relative co-occurence, and
5) information whether the OTU was found to have a parent or not ("No parent found!" or "SETTING XXX to be an ERROR of YYY")

###############################################################################################

In our lab (Aponte's) we have decided to run this step after the LULU curation but many other researchers run it before as well... Our reasoning is that LULU debugs chimeras while the post-clustering groups ASV based on a similarity threshold. LULU reduces the number of ASVs resulting from DADA2 and then the post-clustering makes a more tight reduction. 

```{r 97% post-clustering}
#VSEARCH for post-clustering: to produce the matchlist we used VSEARCH through Anaconda but we can do it also in the pc prompt (Símbolo de Sistema):
# 1) Download the zip file for Windows: https://github.com/torognes/vsearch/releases/download/v2.28.1/vsearch-2.28.1-win-x86_64.zip
# 2) Unzip the file --> find the .exe in the bin folder
# 3) Go to the pc prompt and set folder containing the vsearch exe as the directory: C:\Users\amara.santiesteban> cd "C:\Users\amara.santiesteban\vsearch-2.28.1-win-x86_64 (1)\vsearch-2.28.1-win-x86_64\bin"
# 4) Type in the prompt (replacing "asv_OTU.fasta" for your file name, which should be placed in the SAME DIRECTORY): vsearch.exe --cluster_size asv_OTU.fasta --uc cluster_results.txt --id 0.97 --centroids centroids.fasta
# 5) The output cluster results .txt should be in the same folder

      ### --cluster_size asv_OTU.fasta: This option tells vsearch to cluster the sequences based on their sizes. 
      ### --id 0.97: This sets the similarity threshold for clustering to 97%. Sequences that are 97% similar or more will be clustered together. 
      
###############################################################################################
# In order to do this, first we need to generate a .fasta of our CURATED RESULT after LULU: 
library(Biostrings)

# We have the following objects:
# 1. OTU_table: Original OTU table used in LULU.
# 2. curated_result: Output from the lulu() function.
# 3. seqs: Named vector of sequences, where names correspond to OTU IDs (output from DADA2)

# Get the refined OTU IDs from the LULU output
curated_otu_ids <- rownames(curated_result$curated_table)

# Extract the sequences from the seqtab.nochim object (DADA2 output)
asv_seqs <- colnames(seqtab.nochim)
asv_headers <- vector(dim(seqtab.nochim)[2], mode="character")
for (i in 1:dim(seqtab.nochim)[2]) {
  asv_headers[i] <- paste("ASV", i, sep="_")
}

# Create a named vector of sequences
seqs <- setNames(asv_seqs, asv_headers)

# Extract the sequences corresponding to the curated OTUs
curated_seqs <- seqs[curated_otu_ids]

# Convert to DNAStringSet
curated_DNAStringSet <- DNAStringSet(curated_seqs) #An object that contains the curated sequences ready to be written to a FASTA file.

# Write to a FASTA file
writeXStringSet(curated_DNAStringSet, "Raw_data_firefungi_soil/curated_FASTA.fasta", width = 99999) #adjusting the width will make sure that each sequence is written on a single line. By default, the writeXStringSet() function wraps sequences to 80 characters per line in the output FASTA file.
```

################################################## VSEARCH OUTPUT #####################################################

Reading file curated_FASTA.fasta 100%
1117998 nt in 4397 seqs, min 80, max 445, avg 254
Masking 100%
Sorting by abundance 100%
Counting k-mers 100%
Clustering 100%
Sorting clusters 100%
Writing clusters 100%
*Clusters: 2855* Size min 1, max 14, avg 1.5
Singletons: 2022, 46.0% of seqs, 70.8% of clusters

```{r VSEARCH result}
#After running VSEARCH with the cluster_size and the centroids functions, we will find 2 outputs in the same folder: the cluster_results.txt and the centroids.fasta. 

    ### cluster_results.txt: C=centroids (reference sequence to create the cluster), H= hits; other sequences assigned to the cluster, S= singletones.
    ### centroids.fasta: This FASTA file contains the centroid sequences, which are the representative sequences for each cluster. These centroids are chosen based on the similarity threshold (97% in this case).

clustered_asv.fasta <- readDNAStringSet("F:/FPI_INIA/ADN/Firefungi_2021/Raw_data_firefungi_soil/dada2_output_soil/centroids.fasta")
clustered_asv.fasta

# What we need now is a table with each ASV and its abundance per sample, similar to the curated_asv_table:
# 1) First we will need to identify which ASVs have been clustered in the postclustering
# 2) Then, we will need to sum up their abundances

curated_asv_table <- rownames_to_column(curated_asv_table, var = "centroid_asv")
cluster_results <- read.table("F:/FPI_INIA/ADN/Firefungi_2021/Raw_data_firefungi_soil/dada2_output_soil/cluster_results.txt")
## V2 = the cluster ID. All the ASVs that have been assigned together they will have the same cluster number
```

After this we need to run FUNGuild/Fungal Traits